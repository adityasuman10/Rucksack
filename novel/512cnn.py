# -*- coding: utf-8 -*-
"""512cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19FpYHFROj3qRi-fkqGMxvHLQV6bQJQuR
"""

import os
import numpy as np
import keras
from keras import layers, models, ops
from keras.utils import image_dataset_from_directory

# Dataset path
dataset_path = r"/content/drive/MyDrive/concave"

# Hyperparameters
img_height = 512
img_width = 512
batch_size = 32
epochs = 10

def create_cnn(num_classes):
    """
    Creates a simple CNN with 3 convolutional layers for image classification.

    Args:
        num_classes: Number of output classes for classification

    Returns:
        Keras model
    """
    model = models.Sequential([
        # Input layer
        layers.Input(shape=(img_height, img_width, 1)),

        # First convolutional block
        layers.Conv2D(32, kernel_size=3, strides=2, padding='same'),
        layers.Activation('relu'),
        layers.MaxPooling2D(pool_size=2, strides=2),

        # Second convolutional block
        layers.Conv2D(64, kernel_size=3, strides=2, padding='same'),
        layers.Activation('relu'),
        layers.MaxPooling2D(pool_size=2, strides=2),

        # Third convolutional block
        layers.Conv2D(64, kernel_size=3, strides=2, padding='same'),
        layers.Activation('relu'),
        layers.MaxPooling2D(pool_size=2, strides=2),

        # Flatten before fully connected layers
        layers.Flatten(),

        # Fully connected layer with dropout
        layers.Dense(128),
        layers.Activation('relu'),
        layers.Dropout(0.1),

        # Output layer
        layers.Dense(num_classes),
        layers.Activation('softmax')
    ])

    return model

def load_dataset(dataset_path, validation_split=0.2):
    """
    Load dataset from directory structure.
    Expected structure:
    dataset_path/
        class1/
            image1.png
            image2.png
        class2/
            image1.png
            image2.png
    """
    # Load training dataset
    train_ds = image_dataset_from_directory(
        dataset_path,
        validation_split=validation_split,
        subset="training",
        seed=123,
        image_size=(img_height, img_width),
        batch_size=batch_size,
        color_mode='grayscale',
        label_mode='int'
    )

    # Load validation dataset
    val_ds = image_dataset_from_directory(
        dataset_path,
        validation_split=validation_split,
        subset="validation",
        seed=123,
        image_size=(img_height, img_width),
        batch_size=batch_size,
        color_mode='grayscale',
        label_mode='int'
    )

    # Get class names before processing
    class_names = train_ds.class_names

    # Normalize pixel values to [0, 1]
    normalization_layer = layers.Rescaling(1./255)
    train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
    val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))

    # Optimize performance
    train_ds = train_ds.cache().prefetch(buffer_size=32)
    val_ds = val_ds.cache().prefetch(buffer_size=32)

    return train_ds, val_ds, class_names

if __name__ == "__main__":
    # Check if dataset path exists
    if not os.path.exists(dataset_path):
        print(f"Error: Dataset path '{dataset_path}' does not exist!")
        print("Please update the dataset_path variable with the correct path.")
    else:
        # Load dataset
        print("Loading dataset...")
        train_ds, val_ds, class_names = load_dataset(dataset_path)

        # Get number of classes
        num_classes = len(class_names)
        print(f"Found {num_classes} classes: {class_names}")

        # Create model
        print("\nCreating model...")
        model = create_cnn(num_classes)

        # Compile the model
        model.compile(
            optimizer=keras.optimizers.Adam(),
            loss=keras.losses.SparseCategoricalCrossentropy(),
            metrics=[keras.metrics.SparseCategoricalAccuracy(name='accuracy')]
        )

        # Display model architecture
        model.summary()

        # Train the model
        print("\nTraining model...")
        history = model.fit(
            train_ds,
            validation_data=val_ds,
            epochs=epochs
        )

        # Save the model
        model.save('cnn_model.keras')
        print("\nModel saved as 'cnn_model.keras'")

        # Evaluate on validation set
        print("\nEvaluating model...")
        loss, accuracy = model.evaluate(val_ds)
        print(f"Validation Loss: {loss:.4f}")
        print(f"Validation Accuracy: {accuracy:.4f}")

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

# Get true labels and predicted labels from the validation dataset
true_labels = np.concatenate([y for x, y in val_ds], axis=0)
predicted_probs = model.predict(val_ds)
predicted_labels = np.argmax(predicted_probs, axis=1)

# Generate confusion matrix
conf_matrix = confusion_matrix(true_labels, predicted_labels)
print("Confusion Matrix:")
print(conf_matrix)

# Generate classification report
class_report = classification_report(true_labels, predicted_labels, target_names=class_names)
print("\nClassification Report:")
print(class_report)