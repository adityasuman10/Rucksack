# -*- coding: utf-8 -*-
"""novelCNN2026solar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TuYxqJVhOkNHYgH18hTxmXhwYBWXE-JL
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns
from PIL import Image
import random
import keras
from keras import layers, models, optimizers, callbacks

# Set seeds for reproducibility
random.seed(42)
np.random.seed(42)
keras.utils.set_random_seed(42)

# ============================================================================
# CONFIGURATION - Tune these for your needs
# ============================================================================
IMAGE_SIZE = 128          # Reduced from 224 (2x faster, still captures detail)
NUM_CLASSES = 6
BATCH_SIZE = 8            # Increased from 4 (faster training)
LEARNING_RATE = 5e-4
WEIGHT_DECAY = 1e-4
EPOCHS = 150
DATASET_PATH = r"/content/drive/MyDrive/concave_jellyfish/concave"  # Change to your path

# Model complexity settings
USE_LIGHTWEIGHT = True    # Set False for more accuracy, True for speed
MULTISCALE_STAGES = [1]   # Apply multi-scale only to Stage 1 (fastest)
                          # Use [1, 2] for more accuracy, [1, 2, 3] for maximum

# ============================================================================
# DATA LOADING
# ============================================================================
def create_augmentation_layer():
    """Optimized augmentation for ISAR"""
    return keras.Sequential([
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.08),       # Reduced from 0.15
        layers.RandomZoom(0.12),           # Reduced from 0.15
        layers.RandomContrast(0.15),
    ])

# ============================================================================
# OPTIMIZED MULTI-SCALE CONV (Reduced from 7 to 4 branches)
# ============================================================================
def lightweight_multiscale_conv(x, channels, stride=1):
    """
    Optimized multi-scale for ISAR with only 4 essential branches:
    - 3x3: Standard local features
    - 5x5: Medium-scale scattering points
    - Anisotropic (1x7, 7x1): Linear features (ship edges, aircraft wings)
    """
    # Branch 1: 3x3 - Fast, local features
    branch_3x3 = layers.DepthwiseConv2D(3, strides=stride, padding='same', use_bias=False)(x)
    branch_3x3 = layers.BatchNormalization()(branch_3x3)

    # Branch 2: 5x5 - Medium-scale features
    branch_5x5 = layers.DepthwiseConv2D(5, strides=stride, padding='same', use_bias=False)(x)
    branch_5x5 = layers.BatchNormalization()(branch_5x5)

    # Branch 3: Anisotropic horizontal (1x7) - Horizontal edges/features
    branch_h = layers.DepthwiseConv2D((1, 7), strides=stride, padding='same', use_bias=False)(x)
    branch_h = layers.BatchNormalization()(branch_h)

    # Branch 4: Anisotropic vertical (7x1) - Vertical edges/features
    branch_v = layers.DepthwiseConv2D((7, 1), strides=stride, padding='same', use_bias=False)(x)
    branch_v = layers.BatchNormalization()(branch_v)

    # Concatenate all branches
    multi_scale = layers.Concatenate(axis=-1)([branch_3x3, branch_5x5, branch_h, branch_v])

    # Fuse features with 1x1 conv
    multi_scale = layers.Conv2D(channels, 1, padding='same', use_bias=False)(multi_scale)
    multi_scale = layers.BatchNormalization()(multi_scale)
    multi_scale = layers.Activation('gelu')(multi_scale)

    return multi_scale

# ============================================================================
# ECA BLOCK (Efficient Channel Attention)
# ============================================================================
def eca_block(x, channels, k_size=3):
    """Efficient Channel Attention - 3x faster than SE block"""
    eca = layers.GlobalAveragePooling2D()(x)
    eca = layers.Reshape((channels, 1))(eca)
    eca = layers.Conv1D(1, kernel_size=k_size, padding='same', use_bias=False)(eca)
    eca = layers.Reshape((1, 1, channels))(eca)
    eca = layers.Activation('sigmoid')(eca)
    return layers.Multiply()([x, eca])

# ============================================================================
# OPTIMIZED MBCONV BLOCK
# ============================================================================
def optimized_mbconv_block(x, in_channels, out_channels, expansion_factor=4,
                          stride=1, use_multiscale=False):
    """
    Optimized MBConv with optional multi-scale
    use_multiscale=True: Better accuracy, slower
    use_multiscale=False: Standard conv, faster
    """
    input_tensor = x
    expanded_channels = in_channels * expansion_factor

    # Expansion phase
    if expansion_factor != 1:
        x = layers.Conv2D(expanded_channels, 1, padding='same', use_bias=False)(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('gelu')(x)

    # Depthwise convolution - Choose between multi-scale or standard
    if use_multiscale:
        x = lightweight_multiscale_conv(x, expanded_channels, stride)
    else:
        x = layers.DepthwiseConv2D(3, strides=stride, padding='same', use_bias=False)(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('gelu')(x)

    # ECA attention
    x = eca_block(x, expanded_channels)

    # Projection phase
    x = layers.Conv2D(out_channels, 1, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)

    # Residual connection
    if stride == 1 and in_channels == out_channels:
        x = layers.Add()([input_tensor, x])

    return x

# ============================================================================
# LIGHTWEIGHT TRANSFORMER BLOCK
# ============================================================================
def lightweight_transformer_block(x, in_channels, out_channels, stride=1):
    """Simplified transformer for speed"""
    input_tensor = x

    # Handle channel changes
    if in_channels != out_channels:
        input_tensor = layers.Conv2D(out_channels, 1, use_bias=False)(input_tensor)
        input_tensor = layers.BatchNormalization()(input_tensor)
        x = layers.Conv2D(out_channels, 1, use_bias=False)(x)
        x = layers.BatchNormalization()(x)

    # Handle spatial changes
    if stride > 1:
        x = layers.MaxPooling2D(stride)(x)
        input_tensor = layers.MaxPooling2D(stride)(input_tensor)

    # Lightweight attention (single conv path)
    attn = layers.LayerNormalization()(x)
    attn = layers.Conv2D(out_channels, 3, padding='same')(attn)
    attn = layers.Activation('gelu')(attn)
    attn = layers.Dropout(0.1)(attn)
    x = layers.Add()([x, attn])

    # Lightweight FFN
    ff = layers.LayerNormalization()(x)
    ff = layers.Conv2D(out_channels * 2, 1)(ff)
    ff = layers.Activation('gelu')(ff)
    ff = layers.Conv2D(out_channels, 1)(ff)
    ff = layers.Dropout(0.1)(ff)
    x = layers.Add()([x, ff])

    # Final residual
    x = layers.Add()([input_tensor, x])

    return x

# ============================================================================
# MODEL CREATION
# ============================================================================
def create_optimized_coatnet(num_classes, image_size, use_lightweight=True):
    """
    Optimized CoAtNet for ISAR

    Parameters:
    - use_lightweight: True = ~4M params, False = ~6M params
    """
    inputs = layers.Input(shape=(image_size, image_size, 3))

    # Stem - Lightweight start
    if use_lightweight:
        stem_channels = 32
    else:
        stem_channels = 48

    x = layers.Conv2D(stem_channels, 3, strides=2, padding='same', use_bias=False)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('gelu')(x)



    # Stage 1: MBConv blocks (56x56 or 64x64)
    use_ms_s1 = 1 in MULTISCALE_STAGES
    x = optimized_mbconv_block(x, stem_channels, stem_channels,
                               expansion_factor=3, use_multiscale=use_ms_s1)

    # Stage 2: MBConv with stride (28x28 or 32x32)
    use_ms_s2 = 2 in MULTISCALE_STAGES
    x = optimized_mbconv_block(x, stem_channels, stem_channels * 2,
                               expansion_factor=3, stride=2, use_multiscale=use_ms_s2)

    # Stage 3: Transformer block (14x14 or 16x16)
    x = lightweight_transformer_block(x, stem_channels * 2, stem_channels * 4, stride=2)

    # Stage 4: Final transformer (7x7 or 8x8)
    x = lightweight_transformer_block(x, stem_channels * 4, stem_channels * 6, stride=2)

    # Classification head
    x = layers.LayerNormalization()(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.3)(x)
    x = layers.Dense(num_classes * 2, activation='gelu')(x)
    x = layers.Dropout(0.2)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = models.Model(inputs, outputs)
    return model

# ============================================================================
# TRAINING FUNCTIONS
# ============================================================================
def train_model(model, train_data, val_data, class_names):
    """Train with optimized callbacks"""

    model.compile(
        optimizer=optimizers.AdamW(
            learning_rate=LEARNING_RATE,
            weight_decay=WEIGHT_DECAY
        ),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    print("=" * 70)
    model.summary()
    print("=" * 70)

    # Callbacks
    early_stopping = callbacks.EarlyStopping(
        monitor='val_loss',
        patience=50,              # Reduced from 45 for faster stopping
        restore_best_weights=True,
        verbose=1
    )

    reduce_lr = callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=7,               # More aggressive LR reduction
        min_lr=1e-7,
        verbose=1
    )

    model_checkpoint = callbacks.ModelCheckpoint(
        'best_optimized_model.keras',
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    )

    # Train
    history = model.fit(
        train_data,
        validation_data=val_data,
        epochs=EPOCHS,
        callbacks=[early_stopping, reduce_lr, model_checkpoint],
        verbose=1
    )

    return history

def plot_training_history(history):
    """Plot training metrics"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    # Loss
    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)
    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
    ax1.set_title('Model Loss', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)

    # Accuracy
    ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
    ax2.set_title('Model Accuracy', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy', fontsize=12)
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150)
    plt.show()

def evaluate_model(model, test_data, class_names):
    """Evaluate and create confusion matrix"""

    # Get predictions
    y_pred = model.predict(test_data, verbose=1)
    y_pred_classes = np.argmax(y_pred, axis=1)

    # Get true labels
    y_true = []
    for _, labels in test_data:
        y_true.extend(labels.numpy())
    y_true = np.array(y_true)

    # Metrics
    accuracy = accuracy_score(y_true, y_pred_classes)
    print("\n" + "=" * 70)
    print(f"TEST ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print("=" * 70)
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred_classes, target_names=class_names))

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred_classes)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Count'})
    plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=150)
    plt.show()

    return accuracy, cm

# ============================================================================
# MAIN PIPELINE
# ============================================================================
def main():
    """Main training pipeline"""
    print("\n" + "=" * 70)
    print("OPTIMIZED ISAR CoAtNet - Fast & Accurate")
    print("=" * 70)
    print(f"Configuration:")
    print(f"  Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}")
    print(f"  Batch Size: {BATCH_SIZE}")
    print(f"  Lightweight Mode: {USE_LIGHTWEIGHT}")
    print(f"  Multi-scale Stages: {MULTISCALE_STAGES}")
    print("=" * 70 + "\n")

    # Check dataset path
    if not os.path.exists(DATASET_PATH):
        print(f"❌ Dataset path {DATASET_PATH} does not exist!")
        print("Please update DATASET_PATH in the configuration section.")
        return

    # Load data
    try:
        print("Loading training data...")
        train_dataset = keras.utils.image_dataset_from_directory(
            DATASET_PATH,
            validation_split=0.2,
            subset="training",
            seed=42,
            image_size=(IMAGE_SIZE, IMAGE_SIZE),
            batch_size=BATCH_SIZE,
            label_mode='int'
        )

        print("Loading validation data...")
        val_dataset = keras.utils.image_dataset_from_directory(
            DATASET_PATH,
            validation_split=0.2,
            subset="validation",
            seed=42,
            image_size=(IMAGE_SIZE, IMAGE_SIZE),
            batch_size=BATCH_SIZE,
            label_mode='int'
        )

        class_names = train_dataset.class_names
        num_classes = len(class_names)

        print(f"\n✓ Found {num_classes} classes: {class_names}")

        # Count samples
        train_samples = sum(1 for _ in train_dataset.unbatch())
        val_samples = sum(1 for _ in val_dataset.unbatch())
        print(f"✓ Training samples: {train_samples}")
        print(f"✓ Validation samples: {val_samples}\n")

    except Exception as e:
        print(f"❌ Error loading dataset: {e}")
        return

    # Normalize and optimize
    normalization = layers.Rescaling(1./255)
    train_dataset = train_dataset.map(lambda x, y: (normalization(x), y))
    val_dataset = val_dataset.map(lambda x, y: (normalization(x), y))

    AUTOTUNE = -1
    train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)
    val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)

    # Create model
    print("Creating optimized model...\n")
    model = create_optimized_coatnet(num_classes, IMAGE_SIZE, USE_LIGHTWEIGHT)

    # Train
    print("Starting training...\n")
    history = train_model(model, train_dataset, val_dataset, class_names)

    # Plot results
    print("\nGenerating training plots...")
    plot_training_history(history)

    # Evaluate
    print("\nEvaluating model...")
    accuracy, cm = evaluate_model(model, val_dataset, class_names)

    # Save metrics
    metrics_df = pd.DataFrame({
        'epoch': range(len(history.history['loss'])),
        'train_loss': history.history['loss'],
        'val_loss': history.history['val_loss'],
        'train_accuracy': history.history['accuracy'],
        'val_accuracy': history.history['val_accuracy']
    })
    metrics_df.to_csv('training_metrics.csv', index=False)
    print("\n✓ Metrics saved to 'training_metrics.csv'")

    print("\n" + "=" * 70)
    print("Training completed successfully!")
    print("=" * 70)

if __name__ == "__main__":
    main()